{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from sklearn import feature_selection\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "import spacy\n",
    "import collections\n",
    "import requests\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "import io\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"cah-cards-full.json\")\n",
    "df.drop_duplicates(subset =\"text\")\n",
    "\n",
    "white_sentences = np.array([])\n",
    "for index, row in df.iterrows(): \n",
    "    if row['cardType'] == 'A':\n",
    "        white_sentences = np.append(white_sentences, row['text'])\n",
    "\n",
    "black_sentences = np.array([])\n",
    "for index, row in df.iterrows(): \n",
    "    if row['cardType'] == 'Q' and row['numAnswers'] < 2:\n",
    "        black_sentences = np.append(black_sentences, row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number white sentences :961\n",
      "Number black sentences :161\n"
     ]
    }
   ],
   "source": [
    "print(\"Number white sentences :\"+str(len(white_sentences)))\n",
    "print(\"Number black sentences :\"+str(len(black_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, flg_lemm=True):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(' \\d+', '', text)\n",
    "    text = nlp(np.str(text))\n",
    "    ## remove Stopwords\n",
    "    lst_text = [token for token in text if not token.is_stop]\n",
    "    if flg_lemm:\n",
    "        lst_text = [token.lemma_ for token in lst_text]\n",
    "    return ' '.join(lst_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda x: preprocess_text(x, flg_lemm=True)\n",
    "vfunc = np.vectorize(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_sentences_ = vfunc(white_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_sentences_ = vfunc(black_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_sentences_ = white_sentences_[white_sentences_!='']\n",
    "black_sentences_ = black_sentences_[black_sentences_!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size white:  1473\n",
      "vocab size black:  502\n"
     ]
    }
   ],
   "source": [
    "# count the number of words\n",
    "wc_white = collections.Counter(' '.join(white_sentences_).split())\n",
    "wc_black = collections.Counter(' '.join(black_sentences_).split())\n",
    "\n",
    "# Mapping from index to word : that's the vocabulary\n",
    "vocab_inv_white = [x[0] for x in wc_white.most_common()]\n",
    "vocab_inv_white = list(sorted(vocab_inv_white))\n",
    "\n",
    "# Mapping from index to word : that's the vocabulary\n",
    "vocab_inv_black = [x[0] for x in wc_black.most_common()]\n",
    "vocab_inv_black = list(sorted(vocab_inv_black))\n",
    "\n",
    "# Mapping from word to index\n",
    "vocab_white = {x: i for i, x in enumerate(vocab_inv_white)}\n",
    "words_white = [x[0] for x in wc_white.most_common()]\n",
    "\n",
    "# Mapping from word to index\n",
    "vocab_black = {x: i for i, x in enumerate(vocab_inv_black)}\n",
    "words_black = [x[0] for x in wc_black.most_common()]\n",
    "\n",
    "#size of the vocabulary\n",
    "vocab_white_size = len(words_white)\n",
    "print(\"vocab size white: \", vocab_white_size)\n",
    "vocab_black_size = len(words_black)\n",
    "print(\"vocab size black: \", vocab_black_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER_PATH = '/Users/sofiaadornibraccesi/Downloads/Scraping/chromedriver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "wd = webdriver.Chrome(executable_path=DRIVER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wd.get('https://google.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search_box = wd.find_element_by_css_selector('input.gLFyf')\n",
    "#search_box.send_keys('Dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_image_urls(query:str, max_links_to_fetch:int, wd:webdriver, sleep_between_interactions:int=1):\n",
    "    def scroll_to_end(wd):\n",
    "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(sleep_between_interactions)\n",
    "        \n",
    "    # build the google query\n",
    "    search_url = \"https://www.google.com/search?safe=off&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img\"\n",
    "\n",
    "    # load the page\n",
    "    wd.get(search_url.format(q=query))\n",
    "\n",
    "    image_urls = set()\n",
    "    image_count = 0\n",
    "    results_start = 0\n",
    "    while image_count < max_links_to_fetch:\n",
    "        scroll_to_end(wd)\n",
    "\n",
    "        # get all image thumbnail results\n",
    "        thumbnail_results = wd.find_elements_by_css_selector(\"img.Q4LuWd\")\n",
    "        number_results = len(thumbnail_results)\n",
    "        \n",
    "        print(f\"Found: {number_results} search results. Extracting links from {results_start}:{number_results}\")\n",
    "        \n",
    "        for img in thumbnail_results[results_start:number_results]:\n",
    "            # try to click every thumbnail such that we can get the real image behind it\n",
    "            try:\n",
    "                img.click()\n",
    "                time.sleep(sleep_between_interactions)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            # extract image urls    \n",
    "            actual_images = wd.find_elements_by_css_selector('img.n3VNCb')\n",
    "            for actual_image in actual_images:\n",
    "                if actual_image.get_attribute('src') and 'http' in actual_image.get_attribute('src'):\n",
    "                    image_urls.add(actual_image.get_attribute('src'))\n",
    "\n",
    "            image_count = len(image_urls)\n",
    "\n",
    "            if len(image_urls) >= max_links_to_fetch:\n",
    "                print(f\"Found: {len(image_urls)} image links, done!\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"Found:\", len(image_urls), \"image links, looking for more ...\")\n",
    "            time.sleep(30)\n",
    "            return\n",
    "            load_more_button = wd.find_element_by_css_selector(\".mye4qd\")\n",
    "            if load_more_button:\n",
    "                wd.execute_script(\"document.querySelector('.mye4qd').click();\")\n",
    "\n",
    "        # move the result startpoint further down\n",
    "        results_start = len(thumbnail_results)\n",
    "\n",
    "    return image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_image(folder_path:str,url:str, num=0):\n",
    "    try:\n",
    "        image_content = requests.get(url).content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not download {url} - {e}\")\n",
    "\n",
    "    try:\n",
    "        image_file = io.BytesIO(image_content)\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        file_path = os.path.join(folder_path, str(num)+'.jpg')\n",
    "        with open(file_path, 'wb') as f:\n",
    "            image.save(f, \"JPEG\", quality=85)\n",
    "        print(f\"SUCCESS - saved {url} - as {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not save {url} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_download(search_term:str,driver_path:str,target_path='/Users/sofiaadornibraccesi/Downloads/Scraping/images',number_images=15):\n",
    "    target_folder = os.path.join(target_path,'_'.join(search_term.lower().split(' ')))\n",
    "\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "\n",
    "        with webdriver.Chrome(executable_path=driver_path) as wd:\n",
    "            res = fetch_image_urls(search_term, number_images, wd=wd, sleep_between_interactions=0.5)\n",
    "    \n",
    "        num = 0\n",
    "        if res:\n",
    "            for elem in res:\n",
    "                persist_image(target_folder,elem, num)\n",
    "                num +=1\n",
    "        else: \n",
    "            print(\"None type for links.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#white_sentences_= white_sentences_[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#white_sentences_[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for search_term in white_sentences_: \n",
    "    search_and_download(search_term = search_term, driver_path = DRIVER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path='/Users/sofiaadornibraccesi/Downloads/Scraping/images'\n",
    "for search_term in white_sentences_: \n",
    "    target_folder = os.path.join(target_path,'_'.join(search_term.lower().split(' ')))\n",
    "    if not os.listdir(target_folder):\n",
    "        print(search_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incest\n",
      "kkk\n",
      "heteronormativity\n",
      "dick fingers\n",
      "president george w bush\n",
      "embryonic stem cell\n",
      "auschwitz\n",
      "micropenis\n",
      "clandestine butt scratch\n",
      "forget alamo\n",
      "vagina\n",
      "bite rich person\n",
      "get crush piano\n",
      "money\n",
      "tombus talk rhombus\n",
      "giggle slurp milkshake\n",
      "truck\n",
      "long tongue world\n",
      "wear high heel\n",
      "go commando\n",
      "awful haircut\n"
     ]
    }
   ],
   "source": [
    "target_path='/Users/sofiaadornibraccesi/Downloads/Scraping/images'\n",
    "for search_term in white_sentences_: \n",
    "    target_folder = os.path.join(target_path,'_'.join(search_term.lower().split(' ')))\n",
    "    if len([name for name in os.listdir(target_folder) if os.path.isfile(os.path.join(target_folder, name))])<8: \n",
    "        print(search_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for search_term in black_sentences_: \n",
    "    search_and_download(search_term = search_term, driver_path = DRIVER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
